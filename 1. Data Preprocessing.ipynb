{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89fb414e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78684963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install numpy==1.23.5\n",
    "# !pip install -U pip setuptools wheel\n",
    "# !pip install -U 'spacy[cuda-autodetect]'\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eade3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy requests nlpaug\n",
    "#!pip install torch>=1.6.0 transformers>=4.11.3 sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be58ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import itertools\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_colwidth', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb379bf",
   "metadata": {},
   "source": [
    "# 1. Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c172e8",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51539353",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7780216c",
   "metadata": {},
   "source": [
    "# 1. Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1638c46",
   "metadata": {},
   "source": [
    "## Statistics before Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f300c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_char_before_augmentation = train['facts'].apply(lambda x: len(str(x))).mean()\n",
    "print(f'Average train facts character length (before augmentation): {avg_char_before_augmentation:.0f}')\n",
    "\n",
    "avg_word_before_augmentation = train['facts'].apply(lambda x: len(str(x).split())).mean()\n",
    "print(f'Average train facts word length (before augmentation): {avg_word_before_augmentation:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc023e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_char_before_augmentation = test['facts'].apply(lambda x: len(str(x))).mean()\n",
    "print(f'Average test facts character length (before augmentation): {avg_char_before_augmentation:.0f}')\n",
    "\n",
    "avg_word_before_augmentation = test['facts'].apply(lambda x: len(str(x).split())).mean()\n",
    "print(f'Average test facts word length (before augmentation): {avg_word_before_augmentation:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e1fbf",
   "metadata": {},
   "source": [
    "## Will 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a305dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_apply = ['first_party', 'second_party', 'facts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4759f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_to_apply:\n",
    "    train[column] = train[column].apply(lambda x: re.sub(r'\\bWill\\b', 'Willn', x))\n",
    "\n",
    "for column in columns_to_apply:\n",
    "    test[column] = test[column].apply(lambda x: re.sub(r'\\bWill\\b', 'Willn', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3377b28",
   "metadata": {},
   "source": [
    "## United States 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18388324",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {\n",
    "    'U. S. C.': ' USC ',\n",
    "    'U.S.C.': ' USC ',\n",
    "    'U.S.A.': ' USA ',\n",
    "    'U. S.': ' USA ',\n",
    "    'U.S.': ' USA ',\n",
    "    'US ': ' USA ',\n",
    "    'United States of America': ' USA ',\n",
    "    'United States': ' USA ',\n",
    "    'united states': ' USA '\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17881804",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    train[col] = train[col].replace(replace_dict, regex=True)\n",
    "    \n",
    "for col in test.columns:\n",
    "    test[col] = test[col].replace(replace_dict, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd95177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_whitespaces_func(text):\n",
    "    return re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a857179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a137d",
   "metadata": {},
   "source": [
    "## 한글자 + '.'' 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_Large_and_Large(text):\n",
    "    pattern1 = r'(?<=[A-Z])\\.+\\s+(?=[A-Z]+\\.)'\n",
    "    pattern2 = r'(?<=[A-Z])\\.+(?=[A-Z]+\\.)'\n",
    "    pattern3 = r'([A-Z])\\.'\n",
    "    \n",
    "    \n",
    "    result1 = re.sub(pattern1, '', text)\n",
    "    result2 = re.sub(pattern2, '', result1)\n",
    "    result3 = re.sub(pattern3, lambda match: match.group(1)+' ', result2)    \n",
    "    \n",
    "    return result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e844e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_Large_and_Large))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_Large_and_Large))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38055989",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac35b4c",
   "metadata": {},
   "source": [
    "## & 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_And(text):\n",
    "    pattern1 = r'(?<=[A-Z])\\s+\\&\\s+(?=[A-Z])'\n",
    "    pattern2 = r'(?<=[A-Z])\\&(?=[A-Z])'\\\n",
    "    \n",
    "    result1 = re.sub(pattern1, 'n', text)\n",
    "    result2 = re.sub(pattern2, 'n', result1)\n",
    "    result3 = result2.replace('&',' and ')\n",
    "    \n",
    "    return result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e7783",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_And))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_And))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da0e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da722af4",
   "metadata": {},
   "source": [
    "## 대문자 한글자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_One_Large(text):\n",
    "    text = ' ' + text + ' '\n",
    "    pattern = r'(?<=\\s)[A-Z](?=\\s)'\n",
    "    \n",
    "    result = re.sub(pattern, ' ', text) \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c2c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_One_Large))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_One_Large))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c8279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b249d77e",
   "metadata": {},
   "source": [
    "## Co. Bd. Mt. 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hon_rep = [r'Co\\.', r'CO\\.' , r'Bd\\.', r'Mt\\.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_values_in_column(column, pattern, replacement):\n",
    "    new_column = []\n",
    "    for item in tqdm(column):\n",
    "        new_item = re.sub(pattern, replacement, item)\n",
    "        new_column.append(new_item)\n",
    "    return new_column\n",
    "\n",
    "# 해당 패턴에 맞는 값들을 원하는 replacement 값으로 바꾸는 함수\n",
    "def replace_values_in_train(train, column_name, pattern, replacement):\n",
    "    train[column_name] = replace_values_in_column(train[column_name], pattern, replacement)\n",
    "    return train\n",
    "\n",
    "def replace_values_in_test(test, column_name, pattern, replacement):\n",
    "    test[column_name] = replace_values_in_column(test[column_name], pattern, replacement)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터프레임에서 각 컬럼들에 대해 값을 바꿈\n",
    "for pattern, replacement in zip(Hon_rep, [' Company ', ' Company ', ' Building ', ' Mount ']):\n",
    "    train = replace_values_in_train(train, 'first_party', pattern, replacement)\n",
    "    train = replace_values_in_train(train, 'second_party', pattern, replacement)\n",
    "    train = replace_values_in_train(train, 'facts', pattern, replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터프레임에서 각 컬럼들에 대해 값을 바꿈\n",
    "for pattern, replacement in zip(Hon_rep, [' Company ', ' Company ', ' Building ', ' Mount ']):\n",
    "    test = replace_values_in_test(test, 'first_party', pattern, replacement)\n",
    "    test = replace_values_in_test(test, 'second_party', pattern, replacement)\n",
    "    test = replace_values_in_test(test, 'facts', pattern, replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f5e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca4ada",
   "metadata": {},
   "source": [
    "## INC 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8540a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dot_after_inc1(text):\n",
    "    pattern = r'Inc\\.\\s+([^A-Z\\s]+)'\n",
    "\n",
    "    def replace_dot(match):\n",
    "        return match.group(0).replace(\".\", \"\")\n",
    "\n",
    "    result = re.sub(pattern, replace_dot, text)\n",
    "    return result\n",
    "\n",
    "def remove_dot_after_inc2(text):\n",
    "    pattern = r'Inc\\.([^A-Z\\s]+)'\n",
    "\n",
    "    def replace_dot2(match):\n",
    "        return match.group(0).replace(\".\", \"\")\n",
    "\n",
    "    result = re.sub(pattern, replace_dot2, text)\n",
    "    return result\n",
    "\n",
    "def remove_dot_after_ltd1(text):\n",
    "    pattern = r'Ltd\\.\\s+([^A-Z\\s]+)'\n",
    "\n",
    "    def replace_dot3(match):\n",
    "        return match.group(0).replace(\".\", \"\")\n",
    "\n",
    "    result = re.sub(pattern, replace_dot3, text)\n",
    "    return result\n",
    "\n",
    "def remove_dot_after_ltd2(text):\n",
    "    pattern = r'Ltd\\.([^A-Z\\s]+)'\n",
    "\n",
    "    def replace_dot4(match):\n",
    "        return match.group(0).replace(\".\", \"\")\n",
    "\n",
    "    result = re.sub(pattern, replace_dot4, text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e070a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_inc1))\n",
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_inc2))\n",
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_ltd1))\n",
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_ltd2))\n",
    "\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_inc1))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_inc2))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_ltd1))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_ltd2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74865794",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52a6f77",
   "metadata": {},
   "source": [
    "# 2. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121bf492",
   "metadata": {},
   "source": [
    "## Extracted the nouns from the facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b1ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_str_train = []\n",
    "noun_str_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_str_train = train['facts'].apply(lambda x: [chunk.text for chunk in nlp(x).noun_chunks]).tolist()\n",
    "noun_str_test = test['facts'].apply(lambda x: [chunk.text for chunk in nlp(x).noun_chunks]).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a0b8e3",
   "metadata": {},
   "source": [
    "## Augmented the data using contextual word embeddigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0be815",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0 = train[train['first_party_winner'] == 0].copy()\n",
    "train_0 = pd.concat([train_0]*10, ignore_index=True)\n",
    "train_0['number'] = train_0['ID'].str.replace('TRAIN_', '').astype(int)\n",
    "train_0 = train_0.sort_values('number').reset_index(drop=True)\n",
    "train_0 = train_0.drop('number', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e743baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = train[train['first_party_winner'] == 1].copy()\n",
    "train_1 = pd.concat([train_1]*10, ignore_index=True)\n",
    "train_1['number'] = train_1['ID'].str.replace('TRAIN_', '').astype(int)\n",
    "train_1 = train_1.sort_values('number').reset_index(drop=True)\n",
    "train_1 = train_1.drop('number', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d62e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_str_train_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81949c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(noun_str_train)):\n",
    "    noun_str_train_2.append(list(set(noun_str_train[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_facts_train_0 = []\n",
    "final_facts_train_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_train_0 = train[train['first_party_winner'] == 0].index.tolist()\n",
    "indices_train_1 = train[train['first_party_winner'] == 1].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea6c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(indices_train_0):\n",
    "    aug = naw.SynonymAug(aug_src='wordnet', stopwords=noun_str_train_2[i])\n",
    "    final_facts_train_0.append([train['facts'][i]])\n",
    "    for j in range(9):\n",
    "        augmented_train_0 = aug.augment(train['facts'][i],1,8)\n",
    "        final_facts_train_0.append(augmented_train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a494cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(indices_train_1):\n",
    "    aug = naw.SynonymAug(aug_src='wordnet', stopwords=noun_str_train_2[i])\n",
    "    final_facts_train_1.append([train['facts'][i]])\n",
    "    for j in range(9):\n",
    "        augmented_train_1 = aug.augment(train['facts'][i],1,8)\n",
    "        final_facts_train_1.append(augmented_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_facts_train_0 = [item for sublist in final_facts_train_0 for item in sublist]\n",
    "final_facts_train_1 = [item for sublist in final_facts_train_1 for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48688069",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0['facts'] = final_facts_train_0\n",
    "train_1['facts'] = final_facts_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd838be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train_0,train_1]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4f248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['number'] = train['ID'].str.replace('TRAIN_', '').astype(int)\n",
    "train = train.sort_values('ID').reset_index(drop=True)\n",
    "train = train.drop('number', axis=1)\n",
    "train['ID'] = train.index.map(lambda x: f'TRAIN_{x:04}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c83fb0d",
   "metadata": {},
   "source": [
    "## et al. 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {\n",
    "    \n",
    "    'et. al.': ' ',\n",
    "    'et. al': ' ',\n",
    "    'et al.': ' ',\n",
    "    'et al': ' ',\n",
    "\n",
    "    'at. al.': ' ',\n",
    "    'at. al': ' ',\n",
    "    'at al.': ' ',\n",
    "    'at al': ' ',\n",
    "    \n",
    "    'et. ux.': ' ',\n",
    "    'et. ux': ' ',\n",
    "    'et ux.': ' ',\n",
    "    'et ux': ' ',\n",
    "    \n",
    "    'et. ex.': ' ',\n",
    "    'et. ex': ' ',\n",
    "    'et ex.': ' ',\n",
    "    'et ex': ' ',\n",
    "\n",
    "    'ex. re.': ' ',\n",
    "    'ex. re': ' ',\n",
    "    'ex re.': ' ',\n",
    "    'ex re': ' ',\n",
    "\n",
    "    'et. re.': ' ',\n",
    "    'et. re': ' ',\n",
    "    'et re.': ' ',\n",
    "    'et re': ' ',\n",
    "    \n",
    "    'et. seq.': ' ',\n",
    "    'et. seq': ' ',\n",
    "    'et seq.': ' ',\n",
    "    'et seq': ' ',\n",
    "    \n",
    "    'et. vir.': ' ',\n",
    "    'et. vir': ' ',\n",
    "    'et vir.': ' ',\n",
    "    'et vir': ' ',\n",
    "    \n",
    "    'ex. rel.': ' ',\n",
    "    'ex. rel': ' ',\n",
    "    'ex rel.': ' ',\n",
    "    'ex rel': ' ',\n",
    "    'etc' : ' '\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436a6b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    train[col] = train[col].replace(replace_dict, regex=True)\n",
    "    \n",
    "for col in test.columns:\n",
    "    test[col] = test[col].replace(replace_dict, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33606be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb40e26",
   "metadata": {},
   "source": [
    "## 영어 호칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e947eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Honor = [r'Mr.', r'Mrs.', r'Miss.', r'Dr.', r'Rev.', r'Prof.', r'Capt.', r'Sgt.', r'St.', r'Sr.', r'Jr.', r'Ms.', r'No.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d0862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(train))):\n",
    "    for k in Honor:\n",
    "        train.loc[i, 'facts'] = \" \" + train.loc[i, 'facts'] + \" \"\n",
    "        train.loc[i, 'facts'] = train.loc[i, 'facts'].replace(k,' ')\n",
    "        train.loc[i, 'facts'] = re.sub(r'\\s+', ' ', train.loc[i, 'facts'])\n",
    "        train.loc[i, 'facts'] = re.sub(r\"^\\s+|\\s+$\", \"\", train.loc[i, 'facts'])\n",
    "        \n",
    "        train.loc[i, 'first_party'] = \" \" + train.loc[i, 'first_party'] + \" \"\n",
    "        train.loc[i, 'first_party'] = train.loc[i, 'first_party'].replace(k,' ')\n",
    "        train.loc[i, 'first_party'] = re.sub(r'\\s+', ' ', train.loc[i, 'first_party'])\n",
    "        train.loc[i, 'first_party'] = re.sub(r\"^\\s+|\\s+$\", \"\", train.loc[i, 'first_party'])\n",
    "        \n",
    "        train.loc[i, 'second_party'] = \" \" + train.loc[i, 'second_party'] + \" \"\n",
    "        train.loc[i, 'second_party'] = train.loc[i, 'second_party'].replace(k,' ')\n",
    "        train.loc[i, 'second_party'] = re.sub(r'\\s+', ' ', train.loc[i, 'second_party'])\n",
    "        train.loc[i, 'second_party'] = re.sub(r\"^\\s+|\\s+$\", \"\", train.loc[i, 'second_party'])\n",
    "        \n",
    "        \n",
    "for i in tqdm(range(len(test))):\n",
    "    for k in Honor:\n",
    "        test.loc[i, 'facts'] = \" \" + test.loc[i, 'facts'] + \" \"\n",
    "        test.loc[i, 'facts'] = test.loc[i, 'facts'].replace(k,' ')\n",
    "        test.loc[i, 'facts'] = re.sub(r'\\s+', ' ', test.loc[i, 'facts'])\n",
    "        test.loc[i, 'facts'] = re.sub(r\"^\\s+|\\s+$\", \"\", test.loc[i, 'facts'])\n",
    "        \n",
    "        test.loc[i, 'first_party'] = \" \" + test.loc[i, 'first_party'] + \" \"\n",
    "        test.loc[i, 'first_party'] = test.loc[i, 'first_party'].replace(k,' ')\n",
    "        test.loc[i, 'first_party'] = re.sub(r'\\s+', ' ', test.loc[i, 'first_party'])\n",
    "        test.loc[i, 'first_party'] = re.sub(r\"^\\s+|\\s+$\", \"\", test.loc[i, 'first_party'])\n",
    "        \n",
    "        test.loc[i, 'second_party'] = \" \" + test.loc[i, 'second_party'] + \" \"\n",
    "        test.loc[i, 'second_party'] = test.loc[i, 'second_party'].replace(k,' ')\n",
    "        test.loc[i, 'second_party'] = re.sub(r'\\s+', ' ', test.loc[i, 'second_party'])\n",
    "        test.loc[i, 'second_party'] = re.sub(r\"^\\s+|\\s+$\", \"\", test.loc[i, 'second_party'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de80cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b136d15",
   "metadata": {},
   "source": [
    "## Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c0bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bc9b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_train = []\n",
    "sentence_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96717286",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['facts'] = train['facts'].apply(lambda text: ' '.join([str(sent) + ' [SEP]' for sent in nlp(text).sents]))\n",
    "test['facts'] = test['facts'].apply(lambda text: ' '.join([str(sent) + ' [SEP]' for sent in nlp(text).sents]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7727dd",
   "metadata": {},
   "source": [
    "# 3. Preprocess Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf980c7b",
   "metadata": {},
   "source": [
    "## Statistics before Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd68e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_char_before_preprocessing = train['facts'].apply(lambda x: len(str(x))).mean()\n",
    "print(f'Average train facts character length (before preprocessing): {avg_char_before_preprocessing:.0f}')\n",
    "\n",
    "avg_word_before_preprocessing = train['facts'].apply(lambda x: len(str(x).split())).mean()\n",
    "print(f'Average train facts word length (before preprocessing): {avg_word_before_preprocessing:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1897866",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_char_before_preprocessing = test['facts'].apply(lambda x: len(str(x))).mean()\n",
    "print(f'Average test facts character length (before preprocessing): {avg_char_before_preprocessing:.0f}')\n",
    "\n",
    "avg_word_before_preprocessing = test['facts'].apply(lambda x: len(str(x).split())).mean()\n",
    "print(f'Average test facts word length (before preprocessing): {avg_word_before_preprocessing:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bdc794",
   "metadata": {},
   "source": [
    "## Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fe14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags_func(text):\n",
    "    return BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "\n",
    "def remove_url_func(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "\n",
    "def remove_accented_chars_func(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "\n",
    "def remove_punctuation_func(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "\n",
    "\n",
    "def remove_irr_char_func(text):\n",
    "    return re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "\n",
    "\n",
    "def remove_extra_whitespaces_func(text):\n",
    "    return re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()\n",
    "\n",
    "def remove_english_stopwords_func(text):\n",
    "    t = [token for token in text if token.lower() not in stopwords.words(\"english\")]\n",
    "    text = ' '.join(t)    \n",
    "    return text\n",
    "\n",
    "def norm_lemm_v_a_func(text):\n",
    "    words1 = word_tokenize(text)\n",
    "    text1 = ' '.join([WordNetLemmatizer().lemmatize(word, pos='v') for word in words1])\n",
    "    words2 = word_tokenize(text1)\n",
    "    text2 = ' '.join([WordNetLemmatizer().lemmatize(word, pos='a') for word in words2])\n",
    "    return text2\n",
    "\n",
    "def remove_single_char_func(text, threshold=1):\n",
    "    words = word_tokenize(text)\n",
    "    text = ' '.join([word for word in words if len(word) > threshold])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7813f139",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753d0d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.str.lower())\n",
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_html_tags_func))\n",
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_url_func))\n",
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_accented_chars_func))\n",
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_punctuation_func))\n",
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_irr_char_func))\n",
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
    "train['facts'] = train['facts'].apply(lambda x: x.replace('cls', 'CLS').replace('sep', 'SEP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe383d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.str.lower())\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_html_tags_func))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_url_func))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_accented_chars_func))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_punctuation_func))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_irr_char_func))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
    "test['facts'] = test['facts'].apply(lambda x: x.replace('cls', 'CLS').replace('sep', 'SEP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86215ac2",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f96c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(word_tokenize))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(word_tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bff42ff",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f7ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_english_stopwords_func))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_english_stopwords_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db4ca8f",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20982d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(norm_lemm_v_a_func))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(norm_lemm_v_a_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7240265",
   "metadata": {},
   "source": [
    "## Removing Single Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7986ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_single_char_func))\n",
    "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_single_char_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1bd00",
   "metadata": {},
   "source": [
    "## Statistics after Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d015bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_char_after_preprocessing = train['facts'].apply(lambda x: len(str(x))).mean()\n",
    "print(f'Average train facts character length (after preprocessing): {avg_char_after_preprocessing:.0f}')\n",
    "\n",
    "avg_word_after_preprocessing = train['facts'].apply(lambda x: len(str(x).split())).mean()\n",
    "print(f'Average train facts word length (after preprocessing): {avg_word_after_preprocessing:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f57e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_char_after_preprocessing = test['facts'].apply(lambda x: len(str(x))).mean()\n",
    "print(f'Average test facts character length (after preprocessing): {avg_char_after_preprocessing:.0f}')\n",
    "\n",
    "avg_word_after_preprocessing = test['facts'].apply(lambda x: len(str(x).split())).mean()\n",
    "print(f'Average test facts word length (after preprocessing): {avg_word_after_preprocessing:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f086c1",
   "metadata": {},
   "source": [
    "# 4. Text Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a55b1a9",
   "metadata": {},
   "source": [
    "## Text Exploration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fad11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_word_func(text, n_words=25):\n",
    "    words = word_tokenize(text)\n",
    "    fdist = FreqDist(words) \n",
    "    \n",
    "    n_words = n_words\n",
    "    \n",
    "    df_fdist = pd.DataFrame({'Word': fdist.keys(),\n",
    "                             'Frequency': fdist.values()})\n",
    "    df_fdist = df_fdist.sort_values(by='Frequency', ascending=False).head(n_words)\n",
    "    \n",
    "    return df_fdist\n",
    "\n",
    "def label_func(winner):\n",
    "    if winner == 0:\n",
    "        return 'second_party'\n",
    "    elif winner == 1:\n",
    "        return 'first_party'\n",
    "    \n",
    "def least_common_word_func(text, n_words=25):\n",
    "    words = word_tokenize(text)\n",
    "    fdist = FreqDist(words) \n",
    "    \n",
    "    n_words = n_words\n",
    "    \n",
    "    df_fdist = pd.DataFrame({'Word': fdist.keys(),\n",
    "                             'Frequency': fdist.values()})\n",
    "    df_fdist = df_fdist.sort_values(by='Frequency', ascending=False).tail(n_words)\n",
    "    \n",
    "    return df_fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a9b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = pd.concat([train.iloc[:,:-1],test]).reset_index(drop=True)\n",
    "train_copy = train.copy()\n",
    "test_copy = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d22390",
   "metadata": {},
   "source": [
    "## Most common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93dcfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = df_copy['facts'].str.cat(sep=' ')\n",
    "\n",
    "df_most_common_words_text_corpus = most_common_word_func(text_corpus)\n",
    "\n",
    "df_most_common_words_text_corpus[~df_most_common_words_text_corpus['Word'].isin(['SEP', 'CLS'])].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4535c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "plt.bar(df_most_common_words_text_corpus[~df_most_common_words_text_corpus['Word'].isin(['SEP', 'CLS'])]['Word'], \n",
    "        df_most_common_words_text_corpus[~df_most_common_words_text_corpus['Word'].isin(['SEP', 'CLS'])]['Frequency'])\n",
    "\n",
    "plt.xticks(rotation = 45)\n",
    "\n",
    "plt.xlabel('Most common Words')\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Frequency distribution of the 25 most common words\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c41bc6",
   "metadata": {},
   "source": [
    "## For parts of the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy['Label'] = train_copy['first_party_winner'].apply(label_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88532785",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(train_copy.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "train_copy = train_copy[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f83fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_party = train_copy[(train_copy[\"Label\"] == 'first_party')]['facts']\n",
    "second_party = train_copy[(train_copy[\"Label\"] == 'second_party')]['facts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921230db",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus_first_party = first_party.str.cat(sep=' ')\n",
    "text_corpus_second_party = second_party.str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca25cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_most_common_words_text_corpus_first_party = most_common_word_func(text_corpus_first_party)\n",
    "df_most_common_words_text_corpus_second_party = most_common_word_func(text_corpus_second_party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df5276",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "splited_data = [df_most_common_words_text_corpus_first_party[~df_most_common_words_text_corpus_first_party['Word'].isin(['SEP', 'CLS'])],\n",
    "                df_most_common_words_text_corpus_second_party[~df_most_common_words_text_corpus_second_party['Word'].isin(['SEP', 'CLS'])]]\n",
    "\n",
    "color_list = ['green', 'red']\n",
    "title_list = ['First party', 'Second party']\n",
    "\n",
    "\n",
    "for item in range(2):\n",
    "    plt.figure(figsize=(11,7))\n",
    "    plt.bar(splited_data[item]['Word'], \n",
    "            splited_data[item]['Frequency'],\n",
    "            color=color_list[item])\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.xlabel('Most common Words')\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Frequency distribution of the 25 most common words\")\n",
    "    plt.suptitle(title_list[item], fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4f99cc",
   "metadata": {},
   "source": [
    "# 5. Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9bf6c3",
   "metadata": {},
   "source": [
    "## Data resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2034ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_02 = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_02['first_party'], train_02['second_party'] = train_02['second_party'], train_02['first_party']\n",
    "train_02['first_party_winner'] = 1 - train_02['first_party_winner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ff096",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_02 = pd.concat([train, train_02], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b448f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_02['number'] = train_02['ID'].str.replace('TRAIN_', '').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d953ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_02 = train_02.sort_values(['number', 'first_party_winner']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f702e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_02.drop('number', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ff636",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['ID'] = 'TRAIN_' + train.index.map(lambda x: f'{x:04}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f5294",
   "metadata": {},
   "source": [
    "## Special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a76256",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_to_apply:\n",
    "    train[column] = train[column].apply(lambda x: re.sub(r'\\bSEP\\b', '[SEP]', x))\n",
    "    \n",
    "for column in columns_to_apply:\n",
    "    test[column] = test[column].apply(lambda x: re.sub(r'\\bSEP\\b', '[SEP]', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b48fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['facts'] = '[CLS] ' + train['first_party'] + ' [SEP] ' + train['second_party'] + ' [SEP] ' + train['facts']\n",
    "test['facts'] = '[CLS] ' + test['first_party'] + ' [SEP] ' + test['second_party'] + ' [SEP] ' + test['facts']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c25536",
   "metadata": {},
   "source": [
    "## To_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee92bb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_last_3.csv', index = False)\n",
    "test.to_csv('test_last_3.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
